# ğŸ¦™ Local LLM Server (Ollama + Nginx + Docker)

ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€Docker Compose ã§ **Ollama LLM ã‚µãƒ¼ãƒãƒ¼** ã‚’ç«‹ã¡ä¸Šã’ã‚‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã™ã€‚  
`git clone` ã—ã¦ `docker compose up -d` ã™ã‚‹ã ã‘ã§ API ã‚µãƒ¼ãƒãƒ¼ãŒå‹•ãã¾ã™ã€‚

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †

```bash
# Clone
git clone https://github.com/yourname/ollama-server.git
cd ollama-server

# èµ·å‹•
docker compose up -d

# ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
docker exec -it ollama ollama pull llama3
